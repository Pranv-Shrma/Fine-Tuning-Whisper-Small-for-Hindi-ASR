{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCOSIOKB_o-"
      },
      "outputs": [],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tLwZ3qHDCxl"
      },
      "outputs": [],
      "source": [
        "!uv pip install --quiet transformers\n",
        "!uv pip install --quiet librosa\n",
        "!uv pip install --quiet torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128\n",
        "!uv pip install --quiet datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kJ4UTfXDjMr"
      },
      "outputs": [],
      "source": [
        "# Ensuring the installation of Pytorch\n",
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LT_PU44Dm1D"
      },
      "outputs": [],
      "source": [
        "!uv pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive"
      ],
      "metadata": {
        "id": "TsjX_HgkxmtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xgi4AppEu6K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FWcQD4KDrlo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3aa5fMQDtov"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Whisper_Finetune/FT Data - data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPKVnxDIDw__"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC4mHZ5wDxoK"
      },
      "outputs": [],
      "source": [
        "# Renaming to Hugging Face convention\n",
        "df = df.rename(columns={\n",
        "    'rec_url_gcp': 'audio',\n",
        "    'transcription_url_gcp': 'transcription'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRTDuQURD0Ae"
      },
      "outputs": [],
      "source": [
        "df2 = df[['audio','transcription']]\n",
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the audio files from url path to perform audio processing"
      ],
      "metadata": {
        "id": "86VpT1ZNYipI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVqOTgV2D1jY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def download_audio(row, audio_col=\"audio\", download_dir=\"audio_files\"):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    audio_url = row[audio_col]\n",
        "    basename = os.path.basename(audio_url)\n",
        "    local_path = os.path.join(download_dir, basename)\n",
        "    if not os.path.exists(local_path):\n",
        "        # Download only if not already present\n",
        "        try:\n",
        "            r = requests.get(audio_url)\n",
        "            with open(local_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {audio_url}: {e}\")\n",
        "    return local_path\n",
        "\n",
        "df2[\"audio\"] = df2.apply(download_audio, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyGOnhOND3Vf"
      },
      "outputs": [],
      "source": [
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biuEpcTdD7d_"
      },
      "outputs": [],
      "source": [
        "df2 = df2.rename(columns={\n",
        "    'audio': 'audio_file_path',\n",
        "    'transcription': 'transcription_data'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKfA5MnGD8yz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def is_valid_hindi_text(text):\n",
        "    \"\"\"\n",
        "    Check if text contains only Hindi characters and punctuation marks.\n",
        "    Hindi Unicode range: \\u0900-\\u097F (Devanagari)\n",
        "    Also allows common punctuation and whitespace\n",
        "    \"\"\"\n",
        "    # Define allowed characters: Hindi (Devanagari), whitespace, and common punctuation\n",
        "    pattern = r'^[\\u0900-\\u097F\\s।,.!?;:()\\-]+$'\n",
        "    return bool(re.match(pattern, text))\n",
        "\n",
        "def split_segment_if_needed(segment, max_duration=30):\n",
        "    \"\"\"\n",
        "    Split a segment into multiple parts if it exceeds max_duration.\n",
        "    Returns list of segments with adjusted start/end times.\n",
        "    \"\"\"\n",
        "    duration = segment['end'] - segment['start']\n",
        "\n",
        "    if duration <= max_duration:\n",
        "        return [segment]\n",
        "\n",
        "    # Split into multiple segments\n",
        "    num_parts = int(duration / max_duration) + 1\n",
        "    part_duration = duration / num_parts\n",
        "\n",
        "    segments = []\n",
        "    for i in range(num_parts):\n",
        "        new_segment = segment.copy()\n",
        "        new_segment['start'] = segment['start'] + (i * part_duration)\n",
        "        new_segment['end'] = segment['start'] + ((i + 1) * part_duration)\n",
        "        segments.append(new_segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "def clip_audio_from_transcription(audio_file_path, transcription_data, output_base_dir=\"output\"):\n",
        "    \"\"\"\n",
        "    Clip audio file based on transcription data.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path: Path to the input audio file (local path)\n",
        "        transcription_data: List of dictionaries with start, end, speaker_id, and text\n",
        "        output_base_dir: Base directory in Google Drive for output files\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (stats_dict, segments_list)\n",
        "        - stats_dict: Dictionary with statistics about the processing\n",
        "        - segments_list: List of dictionaries with segment info (clip_path, text)\n",
        "    \"\"\"\n",
        "    # Get audio filename without extension for folder naming\n",
        "    audio_filename = os.path.splitext(os.path.basename(audio_file_path))[0]\n",
        "\n",
        "    # Create output directories specific to this audio file within the Google Drive base directory\n",
        "    audio_output_dir = os.path.join(\"/content/drive/MyDrive/\", output_base_dir, audio_filename)\n",
        "    valid_dir = os.path.join(audio_output_dir, \"valid_clips\")\n",
        "    invalid_dir = os.path.join(audio_output_dir, \"invalid_clips\")\n",
        "    os.makedirs(valid_dir, exist_ok=True)\n",
        "    os.makedirs(invalid_dir, exist_ok=True)\n",
        "\n",
        "    # Load the audio file\n",
        "    print(f\"\\nProcessing: {audio_file_path}\")\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file: {e}\")\n",
        "        return {\n",
        "            \"audio_file\": audio_filename,\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e),\n",
        "            \"valid_clips\": 0,\n",
        "            \"invalid_clips\": 0\n",
        "        }, []\n",
        "\n",
        "    # Track clip counts per speaker\n",
        "    speaker_clip_counts = {}\n",
        "    valid_count = 0\n",
        "    invalid_count = 0\n",
        "    segments_info = []\n",
        "\n",
        "    # Process each transcription segment\n",
        "    for segment in transcription_data:\n",
        "        speaker_id = segment['speaker_id']\n",
        "        text = segment['text']\n",
        "\n",
        "        # Split segment if longer than 30 seconds\n",
        "        split_segments = split_segment_if_needed(segment, max_duration=30)\n",
        "\n",
        "        for sub_segment in split_segments:\n",
        "            start_time = sub_segment['start']\n",
        "            end_time = sub_segment['end']\n",
        "\n",
        "            # Initialize speaker count if not exists\n",
        "            if speaker_id not in speaker_clip_counts:\n",
        "                speaker_clip_counts[speaker_id] = 0\n",
        "\n",
        "            speaker_clip_counts[speaker_id] += 1\n",
        "            clip_number = speaker_clip_counts[speaker_id]\n",
        "\n",
        "            # Create clip filename\n",
        "            clip_filename = f\"{speaker_id}_{clip_number}.wav\"\n",
        "\n",
        "            # Check if text is valid Hindi\n",
        "            is_valid = is_valid_hindi_text(text)\n",
        "\n",
        "            # Output directory selection\n",
        "            if is_valid:\n",
        "                output_path = os.path.join(valid_dir, clip_filename)\n",
        "                valid_count += 1\n",
        "            else:\n",
        "                output_path = os.path.join(invalid_dir, clip_filename)\n",
        "                invalid_count += 1\n",
        "                print(f\"  Invalid text in {clip_filename}: {text[:50]}...\")\n",
        "\n",
        "            # Extract audio clip (convert seconds to milliseconds)\n",
        "            start_ms = int(start_time * 1000)\n",
        "            end_ms = int(end_time * 1000)\n",
        "\n",
        "            try:\n",
        "                audio_clip = audio[start_ms:end_ms]\n",
        "                audio_clip.export(output_path, format=\"wav\")\n",
        "\n",
        "                # Store segment information\n",
        "                segments_info.append({\n",
        "                    \"clip_path\": output_path,\n",
        "                    \"transcript\": text,\n",
        "                    \"speaker_id\": speaker_id,\n",
        "                    \"start_time\": start_time,\n",
        "                    \"end_time\": end_time,\n",
        "                    \"duration\": end_time - start_time,\n",
        "                    \"is_valid_hindi\": is_valid,\n",
        "                    \"original_audio\": audio_filename\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error creating clip {clip_filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "    stats = {\n",
        "        \"audio_file\": audio_filename,\n",
        "        \"status\": \"success\",\n",
        "        \"valid_clips\": valid_count,\n",
        "        \"invalid_clips\": invalid_count,\n",
        "        \"total_clips\": valid_count + invalid_count,\n",
        "        \"speakers\": len(speaker_clip_counts),\n",
        "        \"output_dir\": audio_output_dir\n",
        "    }\n",
        "\n",
        "    print(f\"  ✓ Created {valid_count} valid clips, {invalid_count} invalid clips\")\n",
        "    print(f\"  Output: {audio_output_dir}\")\n",
        "\n",
        "    return stats, segments_info\n",
        "\n",
        "def fetch_transcription_from_url(url):\n",
        "    \"\"\"\n",
        "    Fetch transcription data from a URL (JSON file).\n",
        "    Args:\n",
        "        url: URL to the JSON file containing transcription data\n",
        "    Returns:\n",
        "        List of transcription segments or None if error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        transcription = response.json()\n",
        "        return transcription\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Error fetching transcription from {url}: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"  Error parsing JSON from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_dataframe(df, audio_col=\"audio_file_path\", transcription_col=\"transcription_data\",\n",
        "                     output_base_dir=\"output\"):\n",
        "    \"\"\"\n",
        "    Process a pandas DataFrame with audio files and transcription data.\n",
        "    Args:\n",
        "        df: Pandas DataFrame\n",
        "        audio_col: Name of column containing audio file paths\n",
        "        transcription_col: Name of column containing transcription data URLs or data\n",
        "        output_base_dir: Base directory in Google Drive for all output files\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (summary_df, segments_df)\n",
        "        - summary_df: DataFrame with processing statistics per audio file\n",
        "        - segments_df: DataFrame with each audio segment and its transcript\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    all_segments = []\n",
        "\n",
        "    print(f\"Processing {len(df)} audio files...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        audio_path = row[audio_col]\n",
        "        transcription = row[transcription_col]\n",
        "\n",
        "        # Check if transcription is a URL\n",
        "        if isinstance(transcription, str) and (transcription.startswith('http://') or\n",
        "                                               transcription.startswith('https://')):\n",
        "            print(f\"\\nFetching transcription from URL...\")\n",
        "            transcription = fetch_transcription_from_url(transcription)\n",
        "            if transcription is None:\n",
        "                results.append({\n",
        "                    \"audio_file\": os.path.basename(audio_path),\n",
        "                    \"status\": \"error\",\n",
        "                    \"error\": \"Failed to fetch transcription from URL\",\n",
        "                    \"valid_clips\": 0,\n",
        "                    \"invalid_clips\": 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "        # Handle transcription data if it's a JSON string (not URL)\n",
        "        elif isinstance(transcription, str):\n",
        "            try:\n",
        "                transcription = json.loads(transcription)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing transcription for {audio_path}: {e}\")\n",
        "                results.append({\n",
        "                    \"audio_file\": os.path.basename(audio_path),\n",
        "                    \"status\": \"error\",\n",
        "                    \"error\": \"Invalid JSON in transcription\",\n",
        "                    \"valid_clips\": 0,\n",
        "                    \"invalid_clips\": 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "        # Process the audio file and collect segment info\n",
        "        stats, segments = clip_audio_from_transcription(audio_path, transcription, output_base_dir)\n",
        "        results.append(stats)\n",
        "        all_segments.extend(segments)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Processing Complete!\")\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Create segments DataFrame\n",
        "    segments_df = pd.DataFrame(all_segments)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Total audio files processed: {len(results_df)}\")\n",
        "    print(f\"  Successful: {(results_df['status'] == 'success').sum()}\")\n",
        "    print(f\"  Errors: {(results_df['status'] == 'error').sum()}\")\n",
        "    print(f\"  Total valid clips: {results_df['valid_clips'].sum()}\")\n",
        "    print(f\"  Total invalid clips: {results_df['invalid_clips'].sum()}\")\n",
        "    print(f\"  Total segments: {len(segments_df)}\")\n",
        "\n",
        "    return results_df, segments_df\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    data = df2\n",
        "\n",
        "    # Process all audio files in the DataFrame\n",
        "    # Specify the Google Drive path for output_base_dir\n",
        "    results_df,segment_df = process_dataframe(data, output_base_dir=\"Whisper_Finetune/output_clips\")\n",
        "\n",
        "    # Save results to CSV in Google Drive\n",
        "    results_df.to_csv(\"/content/drive/MyDrive/Whisper_Finetune/processing_results.csv\", index=False)\n",
        "    segment_df.to_csv(\"/content/drive/MyDrive/Whisper_Finetune/segments_results.csv\", index=False)\n",
        "\n",
        "    print(f\"\\nResults saved to: /content/drive/MyDrive/Whisper_Finetune/processing_results.csv and segments_results.csv\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Processing Checkpoint**\n",
        "### Pre processing part has been completed and audio clips for training have been achieved.\n",
        "### New Model trainings can be resumed from this point since the audio data has been processed according to model configuration and saved to disk for future use"
      ],
      "metadata": {
        "id": "lds8RB6aV4df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cggIhnbXV8Hd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "segment_df = pd.read_csv('/content/drive/MyDrive/Whisper_Finetune/segments_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMVTVxiAD-p1"
      },
      "outputs": [],
      "source": [
        "segment_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jov-tVIlEADY"
      },
      "outputs": [],
      "source": [
        "final_df = segment_df[segment_df['is_valid_hindi'] == True]\n",
        "final_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw3jJ6YREBwD"
      },
      "outputs": [],
      "source": [
        "# Logging in to hugging face to upload model\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRyJOHh3EDPm"
      },
      "outputs": [],
      "source": [
        "data_df = final_df[['clip_path', 'transcript']]\n",
        "data_df = data_df.rename(columns={\n",
        "    'clip_path': 'audio',\n",
        "    'transcript': 'sentence'\n",
        "})\n",
        "data_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoFSi1DmTP9J"
      },
      "outputs": [],
      "source": [
        "!uv pip install --quiet evaluate tensorboard librosa gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5oG7WarZNug"
      },
      "outputs": [],
      "source": [
        "!uv pip install --quiet jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTsoQ82ihfqW"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HaeZ7T1ViaD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "from transformers import (\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import librosa\n",
        "\n",
        "# 1. Clearing CUDA cache before model training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Mounting Google Drive for loading data file\n",
        "gdrive_path = \"/content/drive/MyDrive/Whisper_Finetune/\"\n",
        "\n",
        "# 3. Create Hugging Face Dataset from the pandas DataFrame\n",
        "dataset = Dataset.from_pandas(data_df)\n",
        "\n",
        "# Removing the pandas DataFrame to free up memory\n",
        "del data_df\n",
        "\n",
        "# Splitting the dataset into train and test datasets\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "common_voice_datasets = DatasetDict({\n",
        "    \"train\": train_test_split[\"train\"],\n",
        "    \"test\": train_test_split[\"test\"],\n",
        "})\n",
        "\n",
        "# 4. Loading Processor\n",
        "model_name = \"openai/whisper-small\"\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language=\"hindi\", task=\"transcribe\")\n",
        "\n",
        "# 5. Pre-process Data (with on-the-fly audio loading)\n",
        "def prepare_dataset(batch):\n",
        "    # Load and resample audio data from the file path\n",
        "    audio = batch[\"audio\"]\n",
        "    try:\n",
        "        # librosa.load returns a numpy array and the sampling rate\n",
        "        audio_array, sampling_rate = librosa.load(audio, sr=16000)\n",
        "        batch[\"input_features\"] = processor(audio_array, sampling_rate=sampling_rate).input_features[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio}: {e}\")\n",
        "        # In case of an error, you might want to return an empty feature set or handle it appropriately\n",
        "        batch[\"input_features\"] = []\n",
        "\n",
        "\n",
        "    # Process the transcriptions with proper padding token handling\n",
        "    batch[\"labels\"] = processor.tokenizer(\n",
        "        batch[\"sentence\"],\n",
        "        truncation=True,\n",
        "        max_length=448  # Match Whisper's max length\n",
        "    ).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Use .map() to apply the function. Audio is loaded and processed here.\n",
        "# The original 'audio' and 'sentence' columns are kept for now.\n",
        "# You can set remove_columns later if needed.\n",
        "# NEW CODE FOR DEBUGGING\n",
        "print(\"Mapping dataset and processing audio on-the-fly (with a single process for debugging)...\")\n",
        "common_voice_datasets = common_voice_datasets.map(prepare_dataset) # num_proc removed, defaults to 1\n",
        "print(\"Dataset mapping complete.\")\n",
        "\n",
        "\n",
        "# 6. Data Collator and Metrics (same as before)\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels since they need different padding\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # Pad input features\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        # Pad labels using tokenizer in one call (this is the efficient way)\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        # Replace padding with -100 to ignore in loss\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        # Remove decoder_input_ids if present\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# 7. Loading Model\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "# You can remove 'model.config.use_cache = False' as it's not needed for this solution.\n",
        "model.generation_config.language = \"hindi\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "model.generation_config.forced_decoder_ids = None\n",
        "# 8. Define Training Arguments - ALTERNATE SOLUTION\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=gdrive_path + \"whisper-small-hi-custom\",\n",
        "    # 1. Reducing batch size to prevent out-of-memory errors.\n",
        "    per_device_train_batch_size=8,\n",
        "    # 2. Disabling gradient checkpointing, which was caused error.\n",
        "    gradient_checkpointing=False,\n",
        "    # 3. Using gradient accumulation to maintain an effective batch size of 16 (8 * 2).\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    # 4. Changing from 4000 -> 2000 for avoiding overfiting and faster training time\n",
        "    max_steps=2000,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# 9. Initializing Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice_datasets[\"train\"],\n",
        "    eval_dataset=common_voice_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "# 10. Starting Training\n",
        "print(\"\\nStarting fine-tuning on Colab GPU...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# 11. Saving the final model to Google Drive\n",
        "trainer.save_model(gdrive_path + \"whisper-small-hi-custom-final\")\n",
        "print(\"Model saved to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, saving the processor and tokenizer\n",
        "gdrive_path = \"/content/drive/MyDrive/Whisper_Finetune_NEW/\"\n",
        "trainer.save_model(gdrive_path + \"whisper-small-hi-custom-final-new\")\n",
        "processor.save_pretrained(gdrive_path + \"whisper-small-hi-custom-final-new\")\n",
        "print(\"Model and processor saved to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "Xbz9X_0yz_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"Pranav13/whisper-small-hi-custom-final-new\")\n",
        "processor.push_to_hub(\"Pranav13/whisper-small-hi-custom-final-new\")"
      ],
      "metadata": {
        "id": "EIRq77AX0bOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"Pranav13/whisper-small-hi-custom-final-new\")\n",
        "processor.push_to_hub(\"Pranav13/whisper-small-hi-custom-final-new\")"
      ],
      "metadata": {
        "id": "777G0WTi3jRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}